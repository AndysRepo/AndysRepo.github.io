# -*- coding: utf-8 -*-
"""Clustering_tutorial.ipynb

Automatically generated by Colaboratory.

Author: Andrea Basciu (andrea.basciu@dsf.unica.it)
Page: https://molmod.dsf.unica.it/ml_module/

#**Tutorial: K-means clustering of protein conformations**

***Machine Learning Methods in Computational Biophysics***



import matplotlib.pyplot as plt 
import pandas as pd
import numpy as np
import seaborn as sns # data visualization, https://seaborn.pydata.org/

from sklearn.preprocessing import StandardScaler  # Module to scale the dataframe
from sklearn.cluster import KMeans                # To instantiate, train and use model
from sklearn import metrics                       # For Model Evaluation  
#from yellowbrick.cluster import SilhouetteVisualizer

"""Load the dataset (into the *df* variable) using the "[read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)" function. Note the usage of the delimiter (the *sep* parameter). 
Check che [correlation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html) between the variables. Note that the *corr* function uses the linear correlation coefficient by default.


"""

df = pd.read_csv('rog_vs_rmsd_commas.txt', sep=",")

df.corr()

"""Process the dataset, by scaling the features (mean=0, variance=1).

**Why do we need to do it?**


Draw the bloxplots of the scaled distributions with the *seaborn* library.


What is a box plot?

- [Understanding a boxplot](https://towardsdatascience.com/creating-boxplots-with-the-seaborn-python-library-f0c20f09bd57)
- [Comparison Boxplot vs Normal distribution](https://it.wikipedia.org/wiki/Diagramma_a_scatola_e_baffi#/media/File:Boxplot_vs_PDF.svg)
"""

scaler = StandardScaler() # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html
scaled_array = scaler.fit_transform(df)
scaled_dataframe = pd.DataFrame( scaled_array, columns = df.columns )

# Note the usage of the parameter "columns" in the previous line: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.columns.html

#Print both to see the difference

#print(scaled_array)
#print(scaled_dataframe)


plt.figure(figsize = (15,4))
sns.boxplot(data = scaled_dataframe, orient = "h") # https://seaborn.pydata.org/generated/seaborn.boxplot.html from "seaborn" library
plt.show()


scaled_dataframe.describe() # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html | not necessary same definitions used for the boxplots
#scaled_dataframe.info() #https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html

"""[Here](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html
) the link to the *Kmeans* library page

**Note:**

*init (initialization method)*: random/k-means++

*n_init:* 
Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia (i.e.: WCSS - sum of squared distance between each point and the centroid in a cluster).

*max_iter:*
Maximum number of iterations of the k-means algorithm for a single run.

*random_state:*
Determines random number generation for centroid initialization. Use an int to make the randomness deterministic.

"""

km = KMeans(
    n_clusters=5, init='random',
    n_init=10, max_iter=300, 
    tol=1e-04, random_state*=0+2EPS
)

km = KMeans(
    n_clusters=5, init='k-means++',
    n_init=10, max_iter=300, 
    tol=1e-04, random_state=0
)

#km.fit(scaled_dataframe) # to compute the k-means clustering on the scaled dataframe
y_kmeans = km.fit_predict(scaled_dataframe)

centroids = km.cluster_centers_ # save the centroids coordinates into the "centroid" variable

#print(df.iloc[:,0])  # print first feature (RoG) of the "df" dataset
#print(df.iloc[:,1])  # print second feature (RMSD) of the "df" dataset
#print(df.iloc[:1,0])  # print first feature (RoG) of the first instance (index=0) of "df" dataset

# we need "iloc" to specify that we want to read the df by location.
# e.g. read 1 https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html
# e.g. read 2 https://stackoverflow.com/questions/31593201/how-are-iloc-and-loc-different

# Link to the library for scatter plots
#https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html

plt.figure(figsize = (20,10))
plt.scatter(df.iloc[:, 0], df.iloc[:, 1], s=50, c='red')
plt.scatter(scaled_dataframe.iloc[:, 0], scaled_dataframe.iloc[:, 1], s=50, c='blue')
plt.legend(["original" , "scaled"],fontsize ='x-large')

X=scaled_dataframe.iloc # rename the variable just make it shorter

plt.figure(figsize = (22,12))

# For 5 clusters (index from 0 to 4):
# X[y_kmeans==1, 0] gives you the first coordinate (index 0, RoG) of members of cluster 1
# X[y_kmeans==1, 1] gives you the second coordinate (index 1, RMSD) of members of cluster 1
# X[y_kmeans==2, 1] gives you the second coordinate (index 1, RMSD) of members of cluster 2

#e.g.:
#print(X[y_kmeans==1, 0])
# remember X is the scaled dataset
#52     1.438463
#66     0.761834
#63     0.980292
#67     1.020731
#72     1.968820
#73     2.095032
#74     2.013879
#75     2.015896
#82     1.784028
#83     1.684423
#84     1.179382
#239    0.864661

plt.scatter(X[y_kmeans==0, 0], X[y_kmeans==0, 1], s=50, c='lime', label ='Cluster 1')
plt.scatter(X[y_kmeans==1, 0], X[y_kmeans==1, 1], s=50, c='orchid', label ='Cluster 2')
plt.scatter(X[y_kmeans==2, 0], X[y_kmeans==2, 1], s=50, c='wheat', label ='Cluster 3')
plt.scatter(X[y_kmeans==3, 0], X[y_kmeans==3, 1], s=50, c='slateblue', label ='Cluster 4')
plt.scatter(X[y_kmeans==4, 0], X[y_kmeans==4, 1], s=50, c='deepskyblue', label ='Cluster 5')

# Now let's plot the centroids: 
#"centroids[:,0],centroids[:,1] gives you the 0,1 coordinates of all centroids
#
plt.scatter(centroids[:,0],centroids[:,1], s = 300, c = "crimson", label = "centroids", marker="*")
plt.xlabel("RoG (angstrom)",fontsize ='x-large')
plt.ylabel("RMSD (angstrom)",fontsize ='x-large')
plt.legend()
plt.show()

# If interested in using different colors:
# https://matplotlib.org/stable/gallery/color/named_colors.html

# countplot to check the number of protein conformations in each cluster
plt.figure(figsize = (10,5))
plt.ylabel("Count", size=15)
counting = sns.countplot(y_kmeans) # https://seaborn.pydata.org/generated/seaborn.countplot.html

for p in counting.patches:
    height = p.get_height()
    #print(height) #get the counts for each cluster
    counting.text(p.get_x()+p.get_width()/2., height + 0.1 ,ha="center", size=15)

# centroids = km.cluster_centers_



# Calculating distances to identify centroids


# df_elements.apply(lambda x : np.linalg.norm(x-df_center),1)
#
#
#
#

print("## Example: Calculating distances to identify centroids ##")

elements = {'col1': [2,0,1], 'col2': [2, 0,1]} # suppose three different elements (protein conformations)
df_elements = pd.DataFrame(data=elements)

center = {'col1': [2], 'col2': [2]}
df_center = pd.DataFrame(data=center)
print("################ Elements ################ ")
print(df_elements)
print("################ Center ################ ")
print(df_center)


print("################ Distances of the elements from the centers ################")

df_elements['distance_cluster'] = df_elements.apply(lambda x : np.linalg.norm(x-df_center),1)
print(df_elements['distance_cluster'])
print("################ End of the example ################ ")
print("")
print("")
print("")
print("################ Cluster centroids ################ ")
print("")

for i in range(0,5):   # from 0 to 5 actually goes to 0 to 4!
  print('index: ' +str(i)) 
  cluster=scaled_dataframe[y_kmeans ==i] #select the coordinates of the elements of cluster "i"
  print('centroid: ' +str(centroids[i])) # print centroid of cluster "i"
  cluster['distance_centroid'] = cluster.apply(lambda x : np.linalg.norm(x-centroids[i]),1)
  distances_sorted=cluster.sort_values(by=['distance_centroid'], ascending=True) # order distances from smaller one.Ã¬
  representative_structure=distances_sorted[:1]
  index=representative_structure.index.item()
  #print(representative_structure)
  print('index from the original dataset of the cluster representative: ' +str(index))
  print("")

# Implementing the "Elbow method" to look for the optimal number of clusters
# https://en.wikipedia.org/wiki/Elbow_method_(clustering)
# Core idea: look for k* so that:
# k*+1: small decrease in WCSS
# k*-1: high increase in WCSS
# => ELBOW

WCSS = []
for i in range(2,20):  #from 2 to 19 clusters
    model = KMeans(n_clusters = i,init = 'random',n_init=10, max_iter=300, 
    tol=1e-04, random_state=0)
    model.fit(scaled_dataframe)
    WCSS.append(model.inertia_) # the KMeans library already provides you the value for WCSS
fig = plt.figure(figsize = (15,15))
plt.plot(range(2,20),WCSS, linewidth=4, markersize=12,marker='o',color = 'red')
plt.xticks(np.arange(20))
plt.xlabel("Number of clusters", size=15)
plt.ylabel("WCSS", size=15)
plt.show()

# Implementing the "Silhouette method" to look for the optimal number of clusters

km_silhouette = []


for i in range(2,20):
    km = KMeans(n_clusters = i,init = 'random',n_init=10, max_iter=300, 
    tol=1e-04, random_state=0).fit(scaled_dataframe)
    preds = km.predict(scaled_dataframe) # Using the "predict" function of the "Kmeans" library
    
    silhouette = metrics.silhouette_score(scaled_dataframe,preds) # Loaded when we ran: "from sklearn import metrics"   
    km_silhouette.append(silhouette)
    print("Silhouette score for {} clusters: {}".format(i,silhouette))

# Clustering with 3 features, just to show you how to make a 3D plot
#

df = pd.read_csv('rog_vs_rmsd_vs_sasa.txt', sep=",") # New dataset with 3 features, SASA: https://en.wikipedia.org/wiki/Accessible_surface_area

scaler = StandardScaler()
scaled_array = scaler.fit_transform(df)
scaled_dataframe = pd.DataFrame( scaled_array, columns = df.columns )

km = KMeans(
    n_clusters=5, init='random',
    n_init=10, max_iter=300, 
    tol=1e-04, random_state=0
)

km.fit(scaled_dataframe)
centroids = km.cluster_centers_
#print(centroids)
y_kmeans = km.fit_predict(scaled_dataframe)
X=scaled_dataframe.iloc


fig = plt.figure(figsize = (25,25))
ax = fig.add_subplot(111, projection='3d') # This is important, https://jakevdp.github.io/PythonDataScienceHandbook/04.12-three-dimensional-plotting.html

#print(X[y_kmeans==1, 2]) #print 1,0 / 1,1 / 1,2 to check the different features for members of e.g. cluster 1
#print(X[y_kmeans==1,1])

#scatter3D: https://www.geeksforgeeks.org/3d-scatter-plotting-in-python-using-matplotlib/

ax.scatter3D(X[y_kmeans==0, 0],X[y_kmeans==0, 1],X[y_kmeans==0, 2], s=80, c='lime', label ='Cluster 1')
ax.scatter3D(X[y_kmeans==1, 0], X[y_kmeans==1, 1], X[y_kmeans==1, 2], s=80, c='orchid', label ='Cluster 2')
ax.scatter3D(X[y_kmeans==2, 0], X[y_kmeans==2, 1], X[y_kmeans==2, 2], s=80, c='wheat', label ='Cluster 3')
ax.scatter3D(X[y_kmeans==3, 0], X[y_kmeans==3, 1], X[y_kmeans==3, 2], s=80, c='slateblue', label ='Cluster 4')
ax.scatter3D(X[y_kmeans==4, 0], X[y_kmeans==4, 1], X[y_kmeans==4, 2], s=80, c='deepskyblue', label ='Cluster 5')
ax.scatter3D(centroids[:,0],centroids[:,1],centroids[:,2], s = 500, c = "red", label = "centroids", marker="*")

ax.set_xlabel("RoG (angstrom)",fontsize ='x-large')
ax.set_ylabel("RMSD (angstrom)",fontsize ='x-large')
ax.set_zlabel("SASA (angstrom^2)",fontsize ='x-large')

#ax.set_xlim(-2,2.2)

#ax.set_ylim(-1,2.6)

ax.set_zlim(-3,3)

ax.legend()
plt.show()


km_silhouette = []


for i in range(2,20):
    km = KMeans(n_clusters = i,init = 'random',n_init=10, max_iter=300, 
    tol=1e-04, random_state=0).fit(scaled_dataframe)
    preds = km.predict(scaled_dataframe)
    
    silhouette = metrics.silhouette_score(scaled_dataframe,preds)
    km_silhouette.append(silhouette)
    print("Silhouette score for {} clusters: {}".format(i,silhouette))

"""# **K-means++ inizialization**"""

from sklearn.cluster import kmeans_plusplus
from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt

# Generate sample data
n_samples = 4000
n_components = 4 #pay attention to this!

X, y_true = make_blobs(
    n_samples=n_samples, centers=n_components, cluster_std=0.60, random_state=0
)

# X = X[:, ::-1]
# Try with and without it!

# Calculate seeds from kmeans++
centers_init, indices = kmeans_plusplus(X, n_clusters=4, random_state=0)

# Plot init seeds along side sample data
plt.figure(figsize = (20,10))
plt.figure(1)
colors = ["#4EACC5", "#FF9C34", "#4E9A06", "m"]

for k, col in enumerate(colors): #enumerate!
    cluster_data = y_true == k
    plt.scatter(X[cluster_data, 0], X[cluster_data, 1], c=col, marker=".", s=10)


plt.scatter(centers_init[:, 0], centers_init[:, 1], c="b", s=100)
plt.title("K-Means++ Initialization")
plt.xticks([])
plt.yticks([])
plt.show()

"""# **HOMEWORK**

---
##Repeat all the steps needed for a succesful clustering on  your own dataset(s) generated using the different functions of scikit-learn to make clusters of variated shape. Play a little bit with the parameters to understand what you are actually doing!

"""

# https://scikit-learn.org/0.15/modules/generated/sklearn.datasets.make_blobs.html#sklearn.datasets.make_blobs
# https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html

import sklearn
import scipy
import seaborn
import matplotlib.pyplot as plt
import seaborn
from sklearn.datasets import make_blobs

X, y_true = make_blobs(n_samples=1000, centers=3,  
                       cluster_std=0.1, random_state=0) #play with these parameters!
#plt.figure(figsize=(20,12))
#plt.scatter(X[:, 0], X[:, 1], s=50); # s is the size of the dots in the plot



####

from sklearn import cluster, datasets

n_samples=1000

X, y_noisy_circles = datasets.make_circles(n_samples=n_samples, factor=0.5, noise=0.05)
#plt.figure(figsize=(20,12))
#plt.scatter(X[:, 0], X[:, 1], s=50); # s is the size of the dots in the plot


X, Y_noisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05)
plt.figure(figsize=(20,12))
plt.scatter(X[:, 0], X[:, 1], s=50); # s is the size of the dots in the plot

"""# **BONUS (not needed for the exam)**
##Color Quantization in a picture using K-Means Clustering
"""



import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as img
from sklearn.cluster import KMeans
from sklearn.metrics import pairwise_distances_argmin
from sklearn.datasets import load_sample_image # might not be needed!!!!
from sklearn.utils import shuffle
from time import time
import cv2
from skimage import io
from google.colab.patches import cv2_imshow

# If you don't want to use any personal picture, you can use one already available in the 
# "from sklearn.datasets import load_sample_image" library
# Load the Summer Palace photo
#picture = load_sample_image("china.jpg")
# and go on with the tutorial

#Here we are going to use our personal picture
picture=img.imread('/content/nightmare.jpg')

#print(picture.shape)
# for a picture I used: (161, 313, 3)
# 3 is the depth => 3 color channels, RGB picture, Red, Green, Blue (0/255 each color)
#print(picture)
#[[[146 186 194]
#  [145 185 195]
#  [143 183 193]
#  ...
#  [141 167 200]
#  [140 166 199]
#  [138 164 197]]
#
# [[136 174 183]
#  [135 173 184]
# [133 171 182]


# Now: Scikit-learn expects 2d num arrays for the training dataset for a fit function
# ValueError: Found array with dim 3. Estimator expected <= 2.
# We need to transform a 3D array into a 2D one!

img = picture.reshape((picture.shape[0] * picture.shape[1],picture.shape[2])) 



k = 5 # number of clusters
clusterized_colors = KMeans(n_clusters = k) # "pick out" the K-means tool from our collection of algorithms
clusterized_colors.fit(img) # apply the model to our data, the image

clusterized_colors.labels_

label_indx = np.arange(0,len(np.unique(clusterized_colors.labels_)) + 1) 
np.histogram(clusterized_colors.labels_, bins=label_indx)
(hist,_) = np.histogram(clusterized_colors.labels_, bins=label_indx)
hist = hist.astype("float")
hist /= hist.sum()
hist
hist_bar = np.zeros((50, 300, 3), dtype = "uint8") 

startX = 0
for (percent, color) in zip(hist,  clusterized_colors.cluster_centers_): 
  endX = startX + (percent * 300) # to match grid
  cv2.rectangle(hist_bar, (int(startX), 0), (int(endX), 50),
      color.astype("uint8").tolist(), -1)
  startX = endX

plt.figure(figsize=(25,25))
plt.subplot(121)
plt.imshow(picture)
plt.subplot(122)
plt.imshow(hist_bar)
plt.show()